{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library Package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = ['col_4', 'col_10', 'col_11', 'col_12', 'col_19', 'col_21']\n",
    "#df[missing_col].describe()\n",
    "#df[missing_col].mode()\n",
    "# col_10: try mode\n",
    "# col_11: try mean\n",
    "# col_12: try mode\n",
    "# col_19: try mode\n",
    "# col_21: try \n",
    "#missing_value_col = df[motherfucker]\n",
    "#missing_value_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1: Missing Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_1              0\n",
      "col_2              0\n",
      "col_3              0\n",
      "col_4          12817\n",
      "col_5              0\n",
      "col_6              0\n",
      "col_7              0\n",
      "col_8              0\n",
      "col_9              0\n",
      "col_10          6004\n",
      "col_11          1799\n",
      "col_12          1661\n",
      "col_13             0\n",
      "col_14             0\n",
      "col_15             0\n",
      "col_16             0\n",
      "col_17             0\n",
      "col_18             0\n",
      "col_19          3836\n",
      "col_20             0\n",
      "col_21            17\n",
      "CreditScore        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "#print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the mean and median to deal with the missing value \n",
    "# We should clean col_4, 10, 11, 12, 19, 20, 21\n",
    "# Since col_20 is categorical value, we use median\n",
    "# Others are numeric features, we use mean\n",
    "# If testing is not well, we can try put most frequent to fill in missing value\n",
    "\n",
    "col_mean_fill = ['col_4', 'col_10', 'col_11', 'col_12', 'col_19', 'col_21']\n",
    "\n",
    "# For numeric avriables\n",
    "for col in col_mean_fill:\n",
    "    df[col].fillna(df[col].median(), inplace = True)\n",
    "    df_test[col].fillna(df_test[col].median(), inplace = True)\n",
    "\n",
    "# For categorical variable\n",
    "df['col_20'].fillna(df['col_20'].median(), inplace = True)\n",
    "df_test['col_20'].fillna(df_test['col_20'].median(), inplace = True)\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_1     0\n",
      "col_2     0\n",
      "col_3     0\n",
      "col_4     0\n",
      "col_5     0\n",
      "col_6     0\n",
      "col_7     0\n",
      "col_8     0\n",
      "col_9     0\n",
      "col_10    0\n",
      "col_11    0\n",
      "col_12    0\n",
      "col_13    0\n",
      "col_14    0\n",
      "col_15    0\n",
      "col_16    0\n",
      "col_17    0\n",
      "col_18    0\n",
      "col_19    0\n",
      "col_20    0\n",
      "col_21    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(df.isnull().sum())\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do one-hot encoding on the categorical data\n",
    "col_to_encoded = ['col_2', 'col_13', 'col_17', 'col_20']\n",
    "df_preprocessed = pd.get_dummies(df, columns = col_to_encoded)\n",
    "df_test_preprocessed = pd.get_dummies(df_test, columns = col_to_encoded)\n",
    "\n",
    "len(df_preprocessed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_preprocessed.drop('CreditScore', axis = 1)\n",
    "y = df_preprocessed['CreditScore']\n",
    "\n",
    "# Oversampling\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "#sme = SMOTEENN(random_state=42)\n",
    "#X_resampled, y_resampled = sme.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 112171\n",
      "testing data set: 28043\n"
     ]
    }
   ],
   "source": [
    "# Get training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print('training data size:', len(X_train))\n",
    "print('testing data set:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the fucking features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建SelectKBest对象，指定互信息作为评估指标\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=15)  # 选择得分最高的10个特征\n",
    "\n",
    "X_train_std_fs = selector.fit_transform(X_train_std, y_train)\n",
    "X_test_std_fs = selector.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA/T-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components = 15, whiten = True)\n",
    "#tsne = TSNE(n_components = 3, random_state = 42)\n",
    "\n",
    "#pca.fit(X_train_std)\n",
    "#X_train_std_pca = pca.transform(X_train_std)\n",
    "#X_test_std_pca = pca.transform(X_test_std)\n",
    "\n",
    "#tsne.fit(X_train_std)\n",
    "#X_train_std_tsne = tsne.transform(X_train_std)\n",
    "#X_test_std_tsne = tsne.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC result: [0 2 2 ... 2 1 1]\n",
      "Misclassified samples: 3871\n",
      "Accuracy: 0.8620\n",
      "F1-score for class 0:0.8086\n",
      "F1-score for class 1:0.9010\n",
      "F1-score for class 2:0.8722\n",
      "Average F1 score across all classes: 0.8606\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Classifier\n",
    "rfc = RandomForestClassifier(n_estimators=200, criterion = 'gini', random_state=42)\n",
    "# Train\n",
    "rfc.fit(X_train_std_fs, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rfc = rfc.predict(X_test_std_fs)\n",
    "\n",
    "# Evaluation\n",
    "print('RFC result:', y_pred_rfc)\n",
    "print('Misclassified samples: %d' %(y_test != y_pred_rfc).sum())\n",
    "print('Accuracy: %.4f' %accuracy_score(y_test, y_pred_rfc)) \n",
    "\n",
    "# Show the F1-score\n",
    "f1_scores = {}\n",
    "for class_label in set(y_pred_rfc):\n",
    "    y_true_f1 = [1 if label == class_label else 0 for label in y_test]\n",
    "    y_pred_f1 = [1 if label == class_label else 0 for label in y_pred_rfc]\n",
    "\n",
    "    f1_scores[class_label] = f1_score(y_true_f1, y_pred_f1)\n",
    "\n",
    "for class_label, f in f1_scores.items():\n",
    "    print(\"F1-score for class {}:{:.4f}\".format(class_label, f))\n",
    "\n",
    "avg_f1_score = sum(f1_scores.values()) / len(f1_scores)\n",
    "print(\"Average F1 score across all classes: {:.4f}\".format(avg_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict test.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the features that we selected before\n",
    "test_data = df_test_preprocessed\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(test_data)\n",
    "test_data_std = sc.transform(test_data)\n",
    "\n",
    "# Select Features\n",
    "test_data_std_fs = selector.transform(test_data_std)\n",
    "\n",
    "fin_y_pred = rfc.predict(test_data_std_fs)\n",
    "fin_y_pred\n",
    "fin_df = pd.DataFrame({'label': fin_y_pred})\n",
    "fin_df.to_csv('rfc_output_0412.csv', index = True, index_label = 'Id')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1d69b23e567d7d0065b0ee7ea90b1ff2a48e881153cedf1c7b2d00f616e254e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
